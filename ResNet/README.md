ResNet (深度残差网络)
====  
#
* 参考文献为KaiMing He的《Deep Residual Learning for Image Recognition》
* 基于之前的一些模型的学习，可以看出卷积神经网络的深度对最后的分类和识别结果有着重要的影响。而且可以看到自2012年，每年ImageNet的ILSVRC冠军的网络模型的深度都在递增。ResNet是由KaiMing He在2015年发表，并基于该模型获得了当年ImageNet detection，ImageNet localization，COCO detection等多个比赛的冠军。
* 在ImageNet数据集中，He使用了一个152层的深度残差网络，虽然该网络的深度是VGG网络的八倍，但复杂度却更低。最后其在该数据集上top-5的错误率为3.57%，这是一个非常好的成绩（人类的平均错误率是5.1%），该篇论文主要的观点便是提出了一个残差结构。
#
1 介绍
-------
* He在论文的开篇提出一个问题：Is learning better networks as easy as stacking more layers? 但事实是如果只是常规的去堆叠网络的话，当网络越来越深，效果反而会越来越差。其中一个重要的原因便是梯度消失/爆炸现象，当网络变得越来越深后，准确率便趋于饱和，然后继续增加准确率反而开始下降，而且这个下降也并不是由过拟合引起的，如下图便证实了这一点，一个常规的网络在深度达到56层后的效果还不如26层。
![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/32524482.jpg) 
* 基于此，He提出一种解决方案：针对一个较浅的网络在为其添加更多的层时，添加的层是恒等映射，其它层是从学习后的较浅模型拷贝过来的。这样的解决方案能使较深的模型不至于产生比较浅的模型更高的误差。
* 在论文中，通过引入一个深度残差学习框架来解决退化问题（degradation，就是指非常深的网络的性能反而比较浅的网络差，而且越深越差）。通过拟合残差映射来代替之前的拟合底层映射，假设我们最后要求的映射为H(x)，但求H(x)并不容易，我们转而去求F(x)≔H(x)-x，然后再通过F(x)+x来求解最后的H(x)。
* 在这里可能很多人会对H和F函数感到不理解，在这里引用一下知乎上关于这两个函数的回答：F是求和前的网络映射，H是从输入到求和后的网络映射。比如把5映射成5.1，那么引入残差前是F(5)=5.1，引入残差后是H(5)=5.1,F(5)=H(5)-5=0.1。引入残差后对输出的变化更加敏感，比如原来是从5.1增加到了5.2，映射F的输出增加了2%。而对于残差结构的网络，映射F是从0.1增加到了0.2，增加的是100%。残差的思想便是去掉相同的主体部分，从而突出微小的变化，可以理解为差分放大器。如下是残差结构，称之为residual block。<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/18-1-12/56922422.jpg)
* F(x)+x可以被认为是一个“shortcut connections”，关于shortcut我们可以看下图：<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/34147465.jpg)
* 在上图中a^([l])到a^([l+2])经过了两层神经网络层，在传统的神经网络的前向传播中，a^([l])首先经过线性操作得到z^([l+1])=w^([l+1]) a^([l])+b^([l+1])，并通过ReLU非线性激活得到a^([l+1])。然后又通过线性操作得到z^([l+2])=w^([l+2]) a^([l+1])+b^([l+2])，同样的ReLU非线性激活得到a^([l+2])。这是常规网络在前向传播中所进行的处理。而shortcut的处理则是它首先将a^([l])复制一份到z^([l+2])之后，然后和z^([l+2])一起进行一个ReLU非线性激活。
* 论文中还提到了该模型随着深度的增加，产生的结果要比之前传统的网络要表现好得多。而且该模型不仅在ImageNet的数据集上表现的很好，在诸如COCO等的数据集上同样有着很好的表现，说明该模型可以被用作一个通用的模型。

#
2 深度残差学习
-------
2.1 残差学习和shortcuts恒等映射

* 残差学习使一个residual block的网络层去近似一个残差函数F(x)≔H(x)-x来代替原本的H(x)，并用F(x)+x来得到H(x)。尽管两种形式最终都能近似拟合成想要的那个函数，但两种方法学习的困难度可能是大有不同的。
* 这种重构的想法来自于，随着深度增加而引发的退化问题这种有悖常理的现象，正如开始中提到的那样，如果增加的网络做的是一个1:1的恒等映射的话，那么深层次的网络没有理由错误率会比浅层的网络高，所以表明了多层非线性函数激活层的叠加最后对拟合函数出现了问题。通过残差学习的结构，如果恒等映射是最优的话，求解器可以简单地将多层非线性连接的权重推向零从而接近恒等映射。
* 对于shortcut connection的引入，它既没有增加多余的参数也没有增加多余的计算复杂度。这一点不仅在实验过程中非常有用而且也是该网络相比于常规网络有更好表现的重要点。有一点需要注意的是，因为shortcut最后会将a^([l])同z^([l+2])相加，所以必须要保证这两个参数的维度是一致的。我们可以从上面的图中看到一个residual block中间是有两层，在这里两层三层甚至更多层都是可以的。但是如果只有一层的话，那y=W_1 x+x，这样做观察不到有任何的优势。还有一点就是，该图中的两层网络都是全连接层网络，这种结构同样适用于卷积神经网络，也就是说这两个全连接网络也可以替换为两层卷积神经网络。
#
2.2 网络架构


* ResNet传统的网络结构是基于VGG网络的基准。VGG网络的特点主要是在卷积层大量使用了3x3的卷积核，并且基于两个简单的设计规则：1.针对相同尺寸的输出特征图，在输出之前所采用的卷积层中每一层filters的数目是相同的；2.如果特征图的尺寸在经过池化层后减半，那么相应的在之后卷积层的每一层中filters的数目增加一倍，以此来保证每层的时间复杂度。ResNet在最后全连接层与VGG网络的一个不同点在，VGG网络是在最后的flatten操作中将上一层max pooling得到的结果经过两个4096个神经元的全连接层最后以一个softmax预测1000个分类，而ResNet则是通过了一个avg pooling层直接跟上一个1000的softmax全连接层进行预测（这是在GoogLeNet中所提到的方法）。
* 在这里介绍一下用一个avg pooling加softmax代替几个全连接层加softmax的好处，首先一点原来全连接层叠加参数量和计算量极大，有时候一个网络的80%-90%的参数都集中在最后几个FC层中（比如VGG网络中第一个FC层的参数就是7x7x4096x512，接近1亿个参数），而ResNet中的替代方案的参数只有（1x1x512x1000，51.2万个参数）；第二点参数一多就容易造成过拟合，这样就导致原来的模型泛化能力很弱；最后一个好处是利用了空间信息，对于图像在空间中的变化更加鲁棒，下面是VGG和ResNet的架构对比图：<br> 
![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/52196762.jpg)

* 上图中间和右图的区别在于增加了shortcut connection，针对输入和输出维度相同时，该结构能被直接使用（右图实线部分）。而对维度增加的情况（右图虚线部分），因为shortcut传输过来的数据维度和没有经过shortcut的数据维度不一致（因为用了pooling层），论文中考虑两种选项：<br>
>	* shortcut仍旧执行恒等映射，用全是0的数据来填充增加维度
>	* 通过使用1x1卷积核来改变维度使两者维度能一致
#
2.3 实现

* 在针对ImageNet数据的实现过程中，每张输入图片的尺寸被重置为[256, 480]之间的随机数，以此来增加数据集，然后和其他网络一样的操作便是针对该图片，或将此图片经过垂直翻转后的图片进行一个224x224的裁剪。在RGB数据的操作上也使用了之前网络的数据增加方法（即改变对比度或颜色等）。训练过程中权重是重头开始训练的，使用了SGD和一个256大小的mini-batch，学习速率起始为0.1当准确率不再增加之后下降十倍。还有一点是ResNet中没有使用dropout，使用了0.0001的权重衰减和0.9的momentum。测试阶段用了10张裁剪的图片进行平均结果输出，同样的测试图片的尺寸也被重置为{224,256,384,480,640}等。

#
3 实验
-------

在实验的过程中，首先采用了一个18层和一个34层的网络进行训练和测试，两个网络有着相同的形式，具体的结果如下图：<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/63990330.jpg)

* 从上面的左图可以看到网络退化现象，一个34层的常规网络最终的训练误差要高于一个18层的网络，尽管这个18层网络可以说是它的一个子网络。He认为优化难度不大可能是由于梯度消失造成的，他们验证了前向传播和反向传播过程中信号都没有消失。推测深层的常规网络可能有指数级低收敛的特性，从而影响了训练误差。
* 在右图中可以看到用ResNet来进行测试，网络架构用的还是一样只不过多了一个shortcut connection。在第一次对比中使用了完全映射，针对维度不一致的问题采取的是通过填充0来保证维度一致，所以相比于左图的网络右边的网络没有增加任何一点参数。可以看到最终四个网络的训练误差如下表所示：

![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/96736967.jpg)

* 没有增加一个参数防止了网络退化问题得到发生，这里便可以窥到shortcut的强大之处。还有一点是，可以在上表中看到18层的常规网络和18层的ResNet的错误率几乎没有差别，但是ResNet会收敛地更加快。
* 接着论文中比较了Identity shortcut和projection shortcut两种处理维度不一致的方法。从上面的介绍中我们已经知道了这两种方法一个是用0填充参数，另一个是通过1x1卷积核改变维度。比较结果如下表所示：<br> 


![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/4087033.jpg)

* 在这里一共采用了三种方法（可以看到上表中也有A、B、C），第一种零填充方法，第二种是针对维度增加的那几层采取projection shortcut，而其它层仍然采用identity shortcut，第三种是所有的shortcut都是projection shortcut。从上表中可以看到三种方法最后的效果，B和C要好于A，C轻微地好于B，但是这三种方法之间的差别很小，说明projection shortcut其实并不是必要的，而且Identity shortcut还能减少一些计算和内存占用。
* 接下去说一个瓶颈结构（Deeper Bottleneck Architectures），如下图ResNet将shortcut中间的两层变为了三层（右图所示）：<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/75506036.jpg)
* 在这个结构中用到了两个1x1卷积层，第一个1x1卷积层所做的操作是降维，如上图中便是将256维降到了64维，然后通过一个3x3卷积层获取特征，第二个1x1卷积层所做的是升维，将输出又变回与输入一致，这样做就可以大大地减少计算复杂度，而且还能保证性能没有什么损失，这种结构很像一个瓶颈的结构
* 论文之后还介绍了ResNet在CIFAR-10数据集上的表现，同样证明了ResNet不会出现像常规网络那样随着深度增加的退化现象。
#
4 总结
-------

* ResNet网络的重点如下： 
>* Residual block和shortcut connection的引入，使网络能够达到更高的层数并且不会发生网络退化现象； 
>* 再一次（GoogLeNet也提到过）证明了用avg pooling+softmax代替几个FC+softmax能带来更小的参数数量和计算量并不容易造成过拟合。
#
引用
-------
[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) 