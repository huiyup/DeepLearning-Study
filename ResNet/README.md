ResNet (深度残差网络)
====  
#
>	参考文献为KaiMing He的《 Deep Residual Learning for Image Recognition》<br> 

>	* 基于之前的一些模型的学习，可以看出卷积神经网络的深度对最后的分类和识别结果有着重要的影响。而且可以看到自2012年，每年ImageNet的ILSVRC冠军的网络模型的深度都在递增。ResNet是由KaiMing He在2015年发表，并基于该模型获得了当年ImageNet detection，ImageNet localization，COCO detection等多个比赛的冠军。
>	* 在ImageNet数据集中，He使用了一个152层的深度残差网络，虽然该网络的深度是VGG网络的八倍，但复杂度却更低。最后其在该数据集上top-5的错误率为3.57%，这是一个非常好的成绩（人类的平均错误率是5.1%），该篇论文主要的观点便是提出了一个残差结构。
#
1 介绍
-------
>	* He在论文的开篇提出一个问题：Is learning better networks as easy as stacking more layers? 但事实是如果只是常规的去堆叠网络的话，当网络越来越深，效果反而会越来越差。其中一个重要的原因便是梯度消失/爆炸现象，当网络变得越来越深后，准确率便趋于饱和，然后继续增加准确率反而开始下降，而且这个下降也并不是由过拟合引起的，如下图便证实了这一点，一个常规的网络在深度达到56层后的效果还不如26层。<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/32524482.jpg) 
>	* 基于此，He提出一种解决方案：针对一个较浅的网络在为其添加更多的层时，添加的层是恒等映射，其它层是从学习后的较浅模型拷贝过来的。这样的解决方案能使较深的模型不至于产生比较浅的模型更高的误差。
>	* 在论文中，通过引入一个深度残差学习框架来解决退化问题（degradation，就是指非常深的网络的性能反而比较浅的网络差，而且越深越差）。通过拟合残差映射来代替之前的拟合底层映射，假设我们最后要求的映射为H(x)，但求H(x)并不容易，我们转而去求F(x)≔H(x)-x，然后再通过F(x)+x来求解最后的H(x)。
>	* 在这里可能很多人会对H和F函数感到不理解，在这里引用一下知乎上关于这两个函数的回答：F是求和前的网络映射，H是从输入到求和后的网络映射。比如把5映射成5.1，那么引入残差前是F(5)=5.1，引入残差后是H(5)=5.1,F(5)=H(5)-5=0.1。引入残差后对输出的变化更加敏感，比如原来是从5.1增加到了5.2，映射F的输出增加了2%。而对于残差结构的网络，映射F是从0.1增加到了0.2，增加的是100%。残差的思想便是去掉相同的主体部分，从而突出微小的变化，可以理解为差分放大器。如下是残差结构，称之为residual block。<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/18-1-12/56922422.jpg)
>	* F(x)+x可以被认为是一个“shortcut connections”，关于shortcut我们可以看下图：<br> 

![](http://oyvr3xxmh.bkt.clouddn.com/17-12-28/34147465.jpg)
>	* 在上图中a^([l])到a^([l+2])经过了两层神经网络层，在传统的神经网络的前向传播中，a^([l])首先经过线性操作得到z^([l+1])=w^([l+1]) a^([l])+b^([l+1])，并通过ReLU非线性激活得到a^([l+1])。然后又通过线性操作得到z^([l+2])=w^([l+2]) a^([l+1])+b^([l+2])，同样的ReLU非线性激活得到a^([l+2])。这是常规网络在前向传播中所进行的处理。而shortcut的处理则是它首先将a^([l])复制一份到z^([l+2])之后，然后和z^([l+2])一起进行一个ReLU非线性激活。
>	* 论文中还提到了该模型随着深度的增加，产生的结果要比之前传统的网络要表现好得多。而且该模型不仅在ImageNet的数据集上表现的很好，在诸如COCO等的数据集上同样有着很好的表现，说明该模型可以被用作一个通用的模型。